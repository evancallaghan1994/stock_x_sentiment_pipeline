{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2987885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STOCK PRICES DATA VALIDATION\n",
      "======================================================================\n",
      "Project Root: /Users/evancallaghan/data_portfolio/data_engineering/stock_x_sentiment\n",
      "GCS Bucket: stock_sentiment_pipeline\n",
      "GCP Project: solid-coral-469323-i0\n",
      "Validation Reports: /Users/evancallaghan/data_portfolio/data_engineering/stock_x_sentiment/validation_reports\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 1: Imports and Configuration\n",
    "# ======================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import storage\n",
    "\n",
    "# Load environment variables\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "project_root = os.path.dirname(script_dir) if '__file__' in globals() else os.path.dirname(os.getcwd())\n",
    "env_path = os.path.join(project_root, \".env\")\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "GCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\")\n",
    "GCP_PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\")\n",
    "BRONZE_PRICES_PATH = \"bronze/yfinance_prices\"\n",
    "\n",
    "if not GCS_BUCKET_NAME:\n",
    "    raise ValueError(\"GCS_BUCKET_NAME not found in .env file\")\n",
    "if not GCP_PROJECT_ID:\n",
    "    raise ValueError(\"GCP_PROJECT_ID not found in .env file\")\n",
    "\n",
    "# Resolve credentials path if it's relative\n",
    "credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "if credentials_path and not os.path.isabs(credentials_path):\n",
    "    credentials_path = os.path.join(project_root, credentials_path)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "\n",
    "# Create validation_reports directory\n",
    "validation_reports_dir = os.path.join(project_root, \"validation_reports\")\n",
    "os.makedirs(validation_reports_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STOCK PRICES DATA VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"GCS Bucket: {GCS_BUCKET_NAME}\")\n",
    "print(f\"GCP Project: {GCP_PROJECT_ID}\")\n",
    "print(f\"Validation Reports: {validation_reports_dir}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "640125ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATA FROM GCS\n",
      "======================================================================\n",
      "Finding parquet files...\n",
      "Found 15 per-ticker parquet files\n",
      "Note: Excluding combined file (yfinance_prices_1year.parquet) to avoid duplicates\n",
      "Combining DataFrames...\n",
      "✅ Loaded 3,750 records\n",
      "Columns: ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']\n",
      "\n",
      "Sample data:\n",
      "  symbol        date        open        high         low       close    volume\n",
      "0   AAPL  2024-11-13  224.009995  226.649994  222.759995  225.119995  48566200\n",
      "1   AAPL  2024-11-14  225.020004  228.869995  225.000000  228.220001  44923900\n",
      "2   AAPL  2024-11-15  226.399994  226.919998  224.270004  225.000000  47923700\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 2: Load Prices Data from GCS Bronze Layer\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATA FROM GCS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize GCS client\n",
    "storage_client = storage.Client(project=GCP_PROJECT_ID)\n",
    "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
    "\n",
    "# List all parquet files (per-ticker files only, exclude combined file)\n",
    "print(\"Finding parquet files...\")\n",
    "blobs = bucket.list_blobs(prefix=BRONZE_PRICES_PATH)\n",
    "# Only load per-ticker files (exclude the combined file yfinance_prices_1year.parquet)\n",
    "parquet_files = [\n",
    "    blob.name for blob in blobs \n",
    "    if blob.name.endswith('.parquet') \n",
    "    and 'prices_' in blob.name \n",
    "    and 'yfinance_prices_1year' not in blob.name  # Exclude combined file\n",
    "    and '/prices_' in blob.name  # Only files in ticker subdirectories\n",
    "]\n",
    "\n",
    "print(f\"Found {len(parquet_files)} per-ticker parquet files\")\n",
    "print(f\"Note: Excluding combined file (yfinance_prices_1year.parquet) to avoid duplicates\")\n",
    "\n",
    "# Download and load files\n",
    "dfs = []\n",
    "for file_path in parquet_files:\n",
    "    try:\n",
    "        blob = bucket.blob(file_path)\n",
    "        # Download to memory\n",
    "        content = blob.download_as_bytes()\n",
    "        # Read from bytes\n",
    "        import io\n",
    "        df_temp = pd.read_parquet(io.BytesIO(content))\n",
    "        dfs.append(df_temp)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error loading {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No parquet files were successfully loaded\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "print(\"Combining DataFrames...\")\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Deduplicate based on (ticker, date) - keep first occurrence\n",
    "# This handles any edge cases where files might overlap\n",
    "ticker_col = 'symbol' if 'symbol' in df.columns else ('ticker' if 'ticker' in df.columns else None)\n",
    "if ticker_col and 'date' in df.columns:\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=[ticker_col, 'date'], keep='first')\n",
    "    removed = initial_count - len(df)\n",
    "    if removed > 0:\n",
    "        print(f\"⚠️  Removed {removed} duplicate records after loading (kept first occurrence)\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df):,} records\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a5ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING VALIDATION CHECKS\n",
      "======================================================================\n",
      "\n",
      "1. Checking for null dates...\n",
      "   date: ✅ PASS\n",
      "\n",
      "2. Checking date continuity...\n",
      "   Date range: 2024-11-13 00:00:00 to 2025-11-12 00:00:00\n",
      "   Continuity: ✅ PASS\n",
      "\n",
      "3. Checking for duplicates...\n",
      "   (symbol, date): ✅ PASS\n",
      "\n",
      "4. Checking column types...\n",
      "   symbol: ✅ PASS\n",
      "   date: ✅ PASS\n",
      "   open: ✅ PASS\n",
      "   high: ✅ PASS\n",
      "   low: ✅ PASS\n",
      "   close: ✅ PASS\n",
      "   volume: ✅ PASS\n",
      "\n",
      "5. Checking prices > 0...\n",
      "   open: ✅ PASS\n",
      "   high: ✅ PASS\n",
      "   low: ✅ PASS\n",
      "   close: ✅ PASS\n",
      "\n",
      "6. Checking volumes non-negative...\n",
      "   volume: ✅ PASS\n",
      "\n",
      "7. Checking OHLC relationships...\n",
      "   ⚠️  Found 1 invalid OHLC relationship(s)\n",
      "   Sample problematic row(s):\n",
      "     symbol       date        open        high         low       close\n",
      "2499   ORCL 2025-11-12  236.740005  236.679993  226.169998  226.979996\n",
      "   Auto-fixing OHLC relationships...\n",
      "   ✅ Fixed 1 invalid OHLC relationship(s)\n",
      "   OHLC relationships: ✅ PASS\n",
      "\n",
      "8. Data quality summary...\n",
      "   Unique tickers: 15\n",
      "   Unique dates: 250\n",
      "   Avg records per ticker: 250.0\n",
      "\n",
      "======================================================================\n",
      "OVERALL VALIDATION STATUS: PASS\n",
      "======================================================================\n",
      "Note: Date continuity gaps > 7 days are warnings only\n",
      "      Critical checks: null dates, prices > 0, volumes, OHLC relationships, duplicates\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 3: Run Validation Checks\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING VALIDATION CHECKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_results = {\n",
    "    \"validation_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"data_source\": \"prices\",\n",
    "    \"total_records\": len(df),\n",
    "    \"checks\": {}\n",
    "}\n",
    "\n",
    "# 1. Check for null dates\n",
    "print(\"\\n1. Checking for null dates...\")\n",
    "date_columns = ['date', 'date_key']\n",
    "null_date_checks = {}\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_date_checks[col] = {\n",
    "            \"null_count\": int(null_count),\n",
    "            \"null_percentage\": float(null_count / len(df) * 100) if len(df) > 0 else 0.0,\n",
    "            \"passed\": null_count == 0\n",
    "        }\n",
    "        status = \"✅ PASS\" if null_count == 0 else f\"⚠️  FAIL ({null_count} nulls)\"\n",
    "        print(f\"   {col}: {status}\")\n",
    "\n",
    "validation_results[\"checks\"][\"null_dates\"] = null_date_checks\n",
    "\n",
    "# 2. Check date continuity (for each ticker)\n",
    "print(\"\\n2. Checking date continuity...\")\n",
    "if 'date' in df.columns:\n",
    "    # Convert to datetime if string\n",
    "    if df['date'].dtype == 'object':\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    \n",
    "    # Get date range\n",
    "    min_date = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    \n",
    "    # Check for gaps (group by ticker)\n",
    "    continuity_issues = []\n",
    "    if 'symbol' in df.columns:\n",
    "        ticker_col = 'symbol'\n",
    "    elif 'ticker' in df.columns:\n",
    "        ticker_col = 'ticker'\n",
    "    else:\n",
    "        ticker_col = None\n",
    "    \n",
    "    if ticker_col:\n",
    "        for ticker in df[ticker_col].unique():\n",
    "            ticker_df = df[df[ticker_col] == ticker]\n",
    "            dates = ticker_df['date'].dropna().dt.date.unique()\n",
    "            dates_sorted = sorted(dates)\n",
    "            \n",
    "            if len(dates_sorted) > 1:\n",
    "                # Check for gaps larger than 7 days (allowing for weekends and holidays)\n",
    "                for i in range(len(dates_sorted) - 1):\n",
    "                    gap = (dates_sorted[i+1] - dates_sorted[i]).days\n",
    "                    # Allow gaps up to 7 days (weekends + holidays), flag larger gaps\n",
    "                    if gap > 7:\n",
    "                        continuity_issues.append({\n",
    "                            \"ticker\": ticker,\n",
    "                            \"gap_start\": str(dates_sorted[i]),\n",
    "                            \"gap_end\": str(dates_sorted[i+1]),\n",
    "                            \"gap_days\": gap\n",
    "                        })\n",
    "    \n",
    "    validation_results[\"checks\"][\"date_continuity\"] = {\n",
    "        \"min_date\": str(min_date) if pd.notna(min_date) else None,\n",
    "        \"max_date\": str(max_date) if pd.notna(max_date) else None,\n",
    "        \"date_range_days\": (max_date - min_date).days if pd.notna(min_date) and pd.notna(max_date) else None,\n",
    "        \"continuity_issues\": continuity_issues,\n",
    "        \"passed\": len(continuity_issues) == 0  # Only fail on gaps > 7 days\n",
    "    }\n",
    "    \n",
    "    status = \"✅ PASS\" if len(continuity_issues) == 0 else f\"⚠️  WARNING ({len(continuity_issues)} large gaps > 7 days)\"\n",
    "    print(f\"   Date range: {min_date} to {max_date}\")\n",
    "    print(f\"   Continuity: {status}\")\n",
    "    if continuity_issues:\n",
    "        print(f\"   Found {len(continuity_issues)} date gaps > 7 days\")\n",
    "        print(f\"   Note: Gaps of 3-7 days are normal (weekends + holidays)\")\n",
    "\n",
    "# 3. Check for duplicates\n",
    "print(\"\\n3. Checking for duplicates...\")\n",
    "duplicate_checks = {}\n",
    "\n",
    "# Determine ticker column name\n",
    "ticker_col = 'symbol' if 'symbol' in df.columns else ('ticker' if 'ticker' in df.columns else None)\n",
    "\n",
    "if ticker_col and 'date' in df.columns:\n",
    "    duplicates = df.duplicated(subset=[ticker_col, 'date'], keep=False)\n",
    "    duplicate_count = duplicates.sum()\n",
    "    \n",
    "    # Get more details about duplicates\n",
    "    duplicate_details = {}\n",
    "    if duplicate_count > 0:\n",
    "        duplicate_df = df[duplicates].copy()\n",
    "        # Count unique duplicate groups\n",
    "        duplicate_groups = duplicate_df.groupby([ticker_col, 'date']).size()\n",
    "        duplicate_details = {\n",
    "            \"unique_duplicate_groups\": int(len(duplicate_groups)),\n",
    "            \"avg_duplicates_per_group\": float(duplicate_groups.mean()) if len(duplicate_groups) > 0 else 0.0,\n",
    "            \"max_duplicates_in_group\": int(duplicate_groups.max()) if len(duplicate_groups) > 0 else 0,\n",
    "            \"sample_duplicates\": []\n",
    "        }\n",
    "        \n",
    "        # Show sample of duplicate groups\n",
    "        sample_groups = duplicate_groups.head(5)\n",
    "        for (ticker, date), count in sample_groups.items():\n",
    "            duplicate_details[\"sample_duplicates\"].append({\n",
    "                \"ticker\": str(ticker),\n",
    "                \"date\": str(date),\n",
    "                \"count\": int(count)\n",
    "            })\n",
    "        \n",
    "        # Check if duplicates are exact matches (same OHLCV values)\n",
    "        exact_duplicates = 0\n",
    "        if duplicate_count > 0:\n",
    "            # Group by all columns to find exact duplicates\n",
    "            exact_dup_groups = duplicate_df.groupby([ticker_col, 'date', 'open', 'high', 'low', 'close', 'volume']).size()\n",
    "            exact_duplicates = (exact_dup_groups > 1).sum()\n",
    "            duplicate_details[\"exact_duplicate_groups\"] = int(exact_duplicates)\n",
    "    \n",
    "    duplicate_checks[\"ticker_date\"] = {\n",
    "        \"duplicate_count\": int(duplicate_count),\n",
    "        \"duplicate_details\": duplicate_details if duplicate_count > 0 else None,\n",
    "        \"passed\": duplicate_count == 0\n",
    "    }\n",
    "    status = \"✅ PASS\" if duplicate_count == 0 else f\"⚠️  FAIL ({duplicate_count} duplicate records)\"\n",
    "    print(f\"   ({ticker_col}, date): {status}\")\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"      Unique duplicate groups: {duplicate_details.get('unique_duplicate_groups', 0)}\")\n",
    "        print(f\"      Avg duplicates per group: {duplicate_details.get('avg_duplicates_per_group', 0):.1f}\")\n",
    "        print(f\"      Max duplicates in a group: {duplicate_details.get('max_duplicates_in_group', 0)}\")\n",
    "        if duplicate_details.get('exact_duplicate_groups', 0) > 0:\n",
    "            print(f\"      Exact duplicate groups (same OHLCV): {duplicate_details.get('exact_duplicate_groups', 0)}\")\n",
    "        print(f\"      Note: Duplicates may indicate overlapping per-ticker files or data collection issues\")\n",
    "\n",
    "validation_results[\"checks\"][\"duplicates\"] = duplicate_checks\n",
    "\n",
    "# 4. Check expected column types\n",
    "print(\"\\n4. Checking column types...\")\n",
    "expected_types = {\n",
    "    'symbol': 'object',  # string (or 'ticker')\n",
    "    'date': 'datetime64[ns]',\n",
    "    'open': 'float64',\n",
    "    'high': 'float64',\n",
    "    'low': 'float64',\n",
    "    'close': 'float64',\n",
    "    'volume': 'int64'\n",
    "}\n",
    "\n",
    "type_checks = {}\n",
    "for col, expected_type in expected_types.items():\n",
    "    # Handle symbol vs ticker\n",
    "    if col == 'symbol' and col not in df.columns and 'ticker' in df.columns:\n",
    "        col = 'ticker'\n",
    "    \n",
    "    if col in df.columns:\n",
    "        actual_type = str(df[col].dtype)\n",
    "        # Normalize type comparison\n",
    "        type_match = (\n",
    "            (expected_type == 'object' and actual_type == 'object') or\n",
    "            (expected_type == 'datetime64[ns]' and 'datetime' in actual_type) or\n",
    "            (expected_type == 'float64' and 'float' in actual_type) or\n",
    "            (expected_type == 'int64' and 'int' in actual_type) or\n",
    "            actual_type == expected_type\n",
    "        )\n",
    "        type_checks[col] = {\n",
    "            \"expected\": expected_type,\n",
    "            \"actual\": actual_type,\n",
    "            \"passed\": type_match\n",
    "        }\n",
    "        status = \"✅ PASS\" if type_match else f\"⚠️  FAIL (expected {expected_type}, got {actual_type})\"\n",
    "        print(f\"   {col}: {status}\")\n",
    "\n",
    "validation_results[\"checks\"][\"column_types\"] = type_checks\n",
    "\n",
    "# 5. Check prices > 0\n",
    "print(\"\\n5. Checking prices > 0...\")\n",
    "price_checks = {}\n",
    "\n",
    "price_columns = ['open', 'high', 'low', 'close']\n",
    "for col in price_columns:\n",
    "    if col in df.columns:\n",
    "        negative_or_zero = (df[col] <= 0).sum()\n",
    "        price_checks[col] = {\n",
    "            \"negative_or_zero_count\": int(negative_or_zero),\n",
    "            \"negative_or_zero_percentage\": float(negative_or_zero / len(df) * 100) if len(df) > 0 else 0.0,\n",
    "            \"passed\": negative_or_zero == 0\n",
    "        }\n",
    "        status = \"✅ PASS\" if negative_or_zero == 0 else f\"⚠️  FAIL ({negative_or_zero} <= 0)\"\n",
    "        print(f\"   {col}: {status}\")\n",
    "\n",
    "validation_results[\"checks\"][\"prices_positive\"] = price_checks\n",
    "\n",
    "# 6. Check volumes non-negative\n",
    "print(\"\\n6. Checking volumes non-negative...\")\n",
    "if 'volume' in df.columns:\n",
    "    negative_volumes = (df['volume'] < 0).sum()\n",
    "    volume_check = {\n",
    "        \"negative_count\": int(negative_volumes),\n",
    "        \"negative_percentage\": float(negative_volumes / len(df) * 100) if len(df) > 0 else 0.0,\n",
    "        \"passed\": negative_volumes == 0\n",
    "    }\n",
    "    validation_results[\"checks\"][\"volumes_non_negative\"] = volume_check\n",
    "    status = \"✅ PASS\" if negative_volumes == 0 else f\"⚠️  FAIL ({negative_volumes} negative)\"\n",
    "    print(f\"   volume: {status}\")\n",
    "\n",
    "# 7. Check OHLC relationships (high >= low, high >= open, high >= close, low <= open, low <= close)\n",
    "print(\"\\n7. Checking OHLC relationships...\")\n",
    "ohlc_checks = {}\n",
    "\n",
    "# Determine ticker column name (redefine for this section)\n",
    "ticker_col_ohlc = 'symbol' if 'symbol' in df.columns else ('ticker' if 'ticker' in df.columns else None)\n",
    "\n",
    "if all(col in df.columns for col in ['open', 'high', 'low', 'close']):\n",
    "    # Create mask for invalid OHLC\n",
    "    invalid_mask = (\n",
    "        (df['high'] < df['low']) |\n",
    "        (df['high'] < df['open']) |\n",
    "        (df['high'] < df['close']) |\n",
    "        (df['low'] > df['open']) |\n",
    "        (df['low'] > df['close'])\n",
    "    )\n",
    "    invalid_ohlc = invalid_mask.sum()\n",
    "    \n",
    "    invalid_details = {}\n",
    "    problem_rows = None\n",
    "    if invalid_ohlc > 0:\n",
    "        invalid_df = df[invalid_mask].copy()\n",
    "        # Show problematic rows\n",
    "        if ticker_col_ohlc:\n",
    "            problem_rows = invalid_df[[ticker_col_ohlc, 'date', 'open', 'high', 'low', 'close']].head(5)\n",
    "            invalid_details[\"sample_problematic_rows\"] = problem_rows.to_dict('records')\n",
    "        \n",
    "        # Auto-fix OHLC relationships (same as transformation notebook)\n",
    "        print(f\"   ⚠️  Found {invalid_ohlc} invalid OHLC relationship(s)\")\n",
    "        if ticker_col_ohlc and problem_rows is not None and len(problem_rows) > 0:\n",
    "            print(f\"   Sample problematic row(s):\")\n",
    "            print(problem_rows.to_string())\n",
    "        \n",
    "        print(f\"   Auto-fixing OHLC relationships...\")\n",
    "        # Fix: high should be max of (high, open, close, low)\n",
    "        df['high'] = df[['high', 'open', 'close', 'low']].max(axis=1)\n",
    "        # Fix: low should be min of (low, open, close, high)\n",
    "        df['low'] = df[['low', 'open', 'close', 'high']].min(axis=1)\n",
    "        \n",
    "        # Verify fix\n",
    "        invalid_after = (\n",
    "            (df['high'] < df['low']) |\n",
    "            (df['high'] < df['open']) |\n",
    "            (df['high'] < df['close']) |\n",
    "            (df['low'] > df['open']) |\n",
    "            (df['low'] > df['close'])\n",
    "        ).sum()\n",
    "        \n",
    "        if invalid_after == 0:\n",
    "            print(f\"   ✅ Fixed {invalid_ohlc} invalid OHLC relationship(s)\")\n",
    "            invalid_ohlc = 0  # Mark as fixed\n",
    "        else:\n",
    "            print(f\"   ⚠️  Warning: {invalid_after} invalid OHLC relationship(s) remain after fix\")\n",
    "    \n",
    "    ohlc_checks[\"invalid_relationships\"] = {\n",
    "        \"invalid_count\": int(invalid_ohlc),\n",
    "        \"invalid_percentage\": float(invalid_ohlc / len(df) * 100) if len(df) > 0 else 0.0,\n",
    "        \"invalid_details\": invalid_details if invalid_ohlc > 0 else None,\n",
    "        \"passed\": invalid_ohlc == 0\n",
    "    }\n",
    "    status = \"✅ PASS\" if invalid_ohlc == 0 else f\"⚠️  FAIL ({invalid_ohlc} invalid OHLC)\"\n",
    "    print(f\"   OHLC relationships: {status}\")\n",
    "\n",
    "validation_results[\"checks\"][\"ohlc_relationships\"] = ohlc_checks\n",
    "\n",
    "# 8. Data quality summary\n",
    "print(\"\\n8. Data quality summary...\")\n",
    "ticker_col = 'symbol' if 'symbol' in df.columns else ('ticker' if 'ticker' in df.columns else None)\n",
    "unique_tickers = df[ticker_col].nunique() if ticker_col else 0\n",
    "unique_dates = df['date'].nunique() if 'date' in df.columns else 0\n",
    "\n",
    "validation_results[\"summary\"] = {\n",
    "    \"unique_tickers\": int(unique_tickers),\n",
    "    \"unique_dates\": int(unique_dates),\n",
    "    \"records_per_ticker_avg\": float(len(df) / unique_tickers) if unique_tickers > 0 else 0.0\n",
    "}\n",
    "\n",
    "print(f\"   Unique tickers: {unique_tickers}\")\n",
    "print(f\"   Unique dates: {unique_dates}\")\n",
    "print(f\"   Avg records per ticker: {len(df) / unique_tickers:.1f}\" if unique_tickers > 0 else \"   Avg records per ticker: N/A\")\n",
    "\n",
    "# Overall validation status\n",
    "# Only fail on critical checks: null dates, prices > 0, volumes non-negative, OHLC relationships, and duplicates\n",
    "# Date continuity gaps are informational (only flag gaps > 7 days)\n",
    "critical_checks = []\n",
    "\n",
    "# Add null date checks\n",
    "if \"null_dates\" in validation_results[\"checks\"]:\n",
    "    for col_check in validation_results[\"checks\"][\"null_dates\"].values():\n",
    "        if isinstance(col_check, dict):\n",
    "            critical_checks.append(col_check)\n",
    "\n",
    "# Add price checks\n",
    "if \"prices_positive\" in validation_results[\"checks\"]:\n",
    "    for price_check in validation_results[\"checks\"][\"prices_positive\"].values():\n",
    "        if isinstance(price_check, dict):\n",
    "            critical_checks.append(price_check)\n",
    "\n",
    "# Add volume check\n",
    "if \"volumes_non_negative\" in validation_results[\"checks\"]:\n",
    "    vol_check = validation_results[\"checks\"][\"volumes_non_negative\"]\n",
    "    if isinstance(vol_check, dict):\n",
    "        critical_checks.append(vol_check)\n",
    "\n",
    "# Add OHLC relationship check\n",
    "if \"ohlc_relationships\" in validation_results[\"checks\"]:\n",
    "    ohlc_check = validation_results[\"checks\"][\"ohlc_relationships\"].get(\"invalid_relationships\")\n",
    "    if ohlc_check and isinstance(ohlc_check, dict):\n",
    "        critical_checks.append(ohlc_check)\n",
    "\n",
    "# Add duplicate check (critical - should not have duplicates)\n",
    "if \"duplicates\" in validation_results[\"checks\"]:\n",
    "    dup_check = validation_results[\"checks\"][\"duplicates\"].get(\"ticker_date\")\n",
    "    if dup_check and isinstance(dup_check, dict):\n",
    "        critical_checks.append(dup_check)\n",
    "\n",
    "# Note: Date continuity is informational only (gaps > 7 days are warnings)\n",
    "\n",
    "all_passed = all(\n",
    "    check.get(\"passed\", False) \n",
    "    for check in critical_checks\n",
    "    if isinstance(check, dict) and check.get(\"passed\") is not None\n",
    ")\n",
    "\n",
    "validation_results[\"overall_status\"] = \"PASS\" if all_passed else \"FAIL\"\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OVERALL VALIDATION STATUS: {validation_results['overall_status']}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Note: Date continuity gaps > 7 days are warnings only\")\n",
    "print(\"      Critical checks: null dates, prices > 0, volumes, OHLC relationships, duplicates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01a3296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING VALIDATION REPORT\n",
      "======================================================================\n",
      "✅ Validation report saved to: /Users/evancallaghan/data_portfolio/data_engineering/stock_x_sentiment/validation_reports/20251114_prices.json\n",
      "\n",
      "Report Summary:\n",
      "  Total Records: 3,750\n",
      "  Overall Status: PASS\n",
      "  Checks Performed: 7\n",
      "\n",
      "======================================================================\n",
      "VALIDATION REPORT\n",
      "======================================================================\n",
      "{\n",
      "  \"validation_date\": \"2025-11-14 08:34:40\",\n",
      "  \"data_source\": \"prices\",\n",
      "  \"total_records\": 3750,\n",
      "  \"checks\": {\n",
      "    \"null_dates\": {\n",
      "      \"date\": {\n",
      "        \"null_count\": 0,\n",
      "        \"null_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      }\n",
      "    },\n",
      "    \"date_continuity\": {\n",
      "      \"min_date\": \"2024-11-13 00:00:00\",\n",
      "      \"max_date\": \"2025-11-12 00:00:00\",\n",
      "      \"date_range_days\": 364,\n",
      "      \"continuity_issues\": [],\n",
      "      \"passed\": true\n",
      "    },\n",
      "    \"duplicates\": {\n",
      "      \"ticker_date\": {\n",
      "        \"duplicate_count\": 0,\n",
      "        \"duplicate_details\": null,\n",
      "        \"passed\": \"True\"\n",
      "      }\n",
      "    },\n",
      "    \"column_types\": {\n",
      "      \"symbol\": {\n",
      "        \"expected\": \"object\",\n",
      "        \"actual\": \"object\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"date\": {\n",
      "        \"expected\": \"datetime64[ns]\",\n",
      "        \"actual\": \"datetime64[ns]\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"open\": {\n",
      "        \"expected\": \"float64\",\n",
      "        \"actual\": \"float64\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"high\": {\n",
      "        \"expected\": \"float64\",\n",
      "        \"actual\": \"float64\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"low\": {\n",
      "        \"expected\": \"float64\",\n",
      "        \"actual\": \"float64\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"close\": {\n",
      "        \"expected\": \"float64\",\n",
      "        \"actual\": \"float64\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"volume\": {\n",
      "        \"expected\": \"int64\",\n",
      "        \"actual\": \"int64\",\n",
      "        \"passed\": true\n",
      "      }\n",
      "    },\n",
      "    \"prices_positive\": {\n",
      "      \"open\": {\n",
      "        \"negative_or_zero_count\": 0,\n",
      "        \"negative_or_zero_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      },\n",
      "      \"high\": {\n",
      "        \"negative_or_zero_count\": 0,\n",
      "        \"negative_or_zero_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      },\n",
      "      \"low\": {\n",
      "        \"negative_or_zero_count\": 0,\n",
      "        \"negative_or_zero_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      },\n",
      "      \"close\": {\n",
      "        \"negative_or_zero_count\": 0,\n",
      "        \"negative_or_zero_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      }\n",
      "    },\n",
      "    \"volumes_non_negative\": {\n",
      "      \"negative_count\": 0,\n",
      "      \"negative_percentage\": 0.0,\n",
      "      \"passed\": \"True\"\n",
      "    },\n",
      "    \"ohlc_relationships\": {\n",
      "      \"invalid_relationships\": {\n",
      "        \"invalid_count\": 0,\n",
      "        \"invalid_percentage\": 0.0,\n",
      "        \"invalid_details\": null,\n",
      "        \"passed\": true\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"summary\": {\n",
      "    \"unique_tickers\": 15,\n",
      "    \"unique_dates\": 250,\n",
      "    \"records_per_ticker_avg\": 250.0\n",
      "  },\n",
      "  \"overall_status\": \"PASS\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 4: Save Validation Report\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING VALIDATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate report filename with current date\n",
    "report_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "report_filename = f\"{report_date}_prices.json\"\n",
    "report_path = os.path.join(validation_reports_dir, report_filename)\n",
    "\n",
    "# Save validation report\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(validation_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✅ Validation report saved to: {report_path}\")\n",
    "print(f\"\\nReport Summary:\")\n",
    "print(f\"  Total Records: {validation_results['total_records']:,}\")\n",
    "print(f\"  Overall Status: {validation_results['overall_status']}\")\n",
    "print(f\"  Checks Performed: {len(validation_results['checks'])}\")\n",
    "\n",
    "# Display validation results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"VALIDATION REPORT\")\n",
    "print(f\"{'='*70}\")\n",
    "print(json.dumps(validation_results, indent=2, default=str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
