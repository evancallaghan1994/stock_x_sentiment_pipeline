{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182b2eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JAVA_HOME set to: /Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/14 07:54:31 WARN Utils: Your hostname, Evans-MacBook-Pro-2.local, resolves to a loopback address: 127.0.0.1; using 192.168.4.54 instead (on interface en0)\n",
      "25/11/14 07:54:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/14 07:54:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/14 07:54:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NEWS SENTIMENT ANALYSIS PIPELINE\n",
      "======================================================================\n",
      "GCS Bucket: stock_sentiment_pipeline\n",
      "BigQuery Project: solid-coral-469323-i0\n",
      "BigQuery Dataset: stock_sentiment\n",
      "BigQuery Table: news_sentiment_silver\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 1: Imports and Configuration\n",
    "# ======================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, min as spark_min, max as spark_max, \n",
    "    when, isnan, isnull, trim, lower, upper, regexp_replace,\n",
    "    to_date, date_format, lit, concat_ws, split, explode,\n",
    "    avg, stddev, collect_list, struct, udf\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import storage, bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "# Sentiment analysis imports (will be used later)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "project_root = os.path.dirname(script_dir) if '__file__' in globals() else os.path.dirname(os.getcwd())\n",
    "env_path = os.path.join(project_root, \".env\")\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "GCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\")\n",
    "BIGQUERY_PROJECT = os.getenv(\"GCP_PROJECT_ID\")\n",
    "BIGQUERY_DATASET = os.getenv(\"BIGQUERY_DATASET\")\n",
    "BIGQUERY_TABLE = os.getenv(\"BIGQUERY_TABLE\")\n",
    "\n",
    "if not GCS_BUCKET_NAME:\n",
    "    raise ValueError(\"GCS_BUCKET_NAME not found in .env file\")\n",
    "if not BIGQUERY_PROJECT:\n",
    "    raise ValueError(\"GCP_PROJECT_ID not found in .env file\")\n",
    "if not BIGQUERY_DATASET:\n",
    "    raise ValueError(\"BIGQUERY_DATASET not found in .env file\")\n",
    "if not BIGQUERY_TABLE:\n",
    "    raise ValueError(\"BIGQUERY_TABLE not found in .env file\")\n",
    "\n",
    "# Resolve credentials path if it's relative\n",
    "credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "if credentials_path and not os.path.isabs(credentials_path):\n",
    "    credentials_path = os.path.join(project_root, credentials_path)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "\n",
    "# Set JAVA_HOME to Java 17 for Spark\n",
    "import subprocess\n",
    "try:\n",
    "    java_home = subprocess.check_output(['/usr/libexec/java_home', '-v', '17'], text=True).strip()\n",
    "    os.environ['JAVA_HOME'] = java_home\n",
    "    os.environ['PATH'] = f\"{java_home}/bin:{os.environ.get('PATH', '')}\"\n",
    "    print(f\"✅ JAVA_HOME set to: {java_home}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Warning: Could not set JAVA_HOME automatically: {e}\")\n",
    "    print(\"   Make sure Java 17+ is installed and JAVA_HOME is set manually\")\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NewsSentimentPipeline\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client(project=BIGQUERY_PROJECT)\n",
    "\n",
    "# Initialize GCS client\n",
    "storage_client = storage.Client(project=BIGQUERY_PROJECT)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NEWS SENTIMENT ANALYSIS PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"GCS Bucket: {GCS_BUCKET_NAME}\")\n",
    "print(f\"BigQuery Project: {BIGQUERY_PROJECT}\")\n",
    "print(f\"BigQuery Dataset: {BIGQUERY_DATASET}\")\n",
    "print(f\"BigQuery Table: {BIGQUERY_TABLE}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f69d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATA FROM GCS\n",
      "======================================================================\n",
      "Bucket: stock_sentiment_pipeline\n",
      "Path: bronze/news/stock_news_api\n",
      "\n",
      "Finding parquet files...\n",
      "Found 15 parquet files\n",
      "Downloading files from GCS...\n",
      "Downloaded 15 files\n",
      "Loading files with Pandas (to handle timestamp types)...\n",
      "Combining DataFrames...\n",
      "Converting data types for Spark compatibility...\n",
      "Dropping columns not needed for pipeline: ['topics']\n",
      "Converting to Spark DataFrame...\n",
      "Caching DataFrame in memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 07:57:49 WARN TaskSetManager: Stage 1 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/14 07:57:51 WARN TaskSetManager: Stage 2 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 23,359 articles\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- news_url: string (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- source_name: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- tickers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- search_source: string (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- query_date: timestamp (nullable = true)\n",
      " |-- endpoint_used: string (nullable = true)\n",
      " |-- search_terms_used: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+-------------+-------------------+---------+-------+-------+-------------+------+------------+-------------------+-------------+-----------------+\n",
      "|                                          news_url|                                         image_url|                                             title|                                              text|  source_name|               date|sentiment|   type|tickers|search_source|ticker|company_name|         query_date|endpoint_used|search_terms_used|\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+-------------+-------------------+---------+-------+-------+-------------+------+------------+-------------------+-------------+-----------------+\n",
      "|https://www.reuters.com/technology/artificial-i...|https://cdn.snapi.dev/images/v1/0/u/7/aapl28-26...|Apple to announce AI wall tablet as soon as Mar...|Apple is planning on launching a wall-mounted d...|      Reuters|2024-11-12 16:30:13| Positive|Article| [AAPL]|       ticker|  AAPL|  Apple Inc.|2024-11-12 00:00:00|     advanced|      ticker:AAPL|\n",
      "|https://nypost.com/2024/11/12/business/apple-re...|https://cdn.snapi.dev/images/v1/p/c/b/aapl8-268...|Apple developing an iPad-like AI device that ca...|The product, code-named J490, could be announce...|New York Post|2024-11-12 19:27:13|  Neutral|Article| [AAPL]|       ticker|  AAPL|  Apple Inc.|2024-11-12 00:00:00|     advanced|      ticker:AAPL|\n",
      "|https://www.cnbc.com/2024/11/12/apple-wont-laun...|https://cdn.snapi.dev/images/v1/c/0/t/aapl30-26...|Apple won't launch a smart ring, says Oura CEO:...|Apple will not introduce a smart ring, the CEO ...|         CNBC|2024-11-12 07:35:34|  Neutral|Article| [AAPL]|       ticker|  AAPL|  Apple Inc.|2024-11-12 00:00:00|     advanced|      ticker:AAPL|\n",
      "|https://techcrunch.com/2024/11/12/apple-reporte...|https://cdn.snapi.dev/images/v1/w/c/j/aapl4-248...|Apple reportedly shipping a security camera in ...|Apple is set to expand its smart home presence,...|   TechCrunch|2024-11-12 13:42:01| Positive|Article| [AAPL]|       ticker|  AAPL|  Apple Inc.|2024-11-12 00:00:00|     advanced|      ticker:AAPL|\n",
      "|https://techcrunch.com/2024/11/12/apple-reporte...|https://cdn.snapi.dev/images/v1/k/y/o/aapl29-26...|Apple reportedly releasing a wall-mounted smart...|Apple is gearing up to announce a new smart hom...|   TechCrunch|2024-11-12 18:53:38| Positive|Article| [AAPL]|       ticker|  AAPL|  Apple Inc.|2024-11-12 00:00:00|     advanced|      ticker:AAPL|\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+-------------+-------------------+---------+-------+-------+-------------+------+------------+-------------------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "✅ Cleaned up temporary files\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 2: Load Data from GCS Bronze Layer\n",
    "# ======================================================================\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "BRONZE_NEWS_PATH = \"bronze/news/stock_news_api\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATA FROM GCS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Bucket: {GCS_BUCKET_NAME}\")\n",
    "print(f\"Path: {BRONZE_NEWS_PATH}\")\n",
    "print()\n",
    "\n",
    "# Initialize GCS client\n",
    "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
    "\n",
    "# List all parquet files\n",
    "print(\"Finding parquet files...\")\n",
    "blobs = bucket.list_blobs(prefix=BRONZE_NEWS_PATH)\n",
    "parquet_files = [blob.name for blob in blobs if blob.name.endswith('.parquet')]\n",
    "\n",
    "print(f\"Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "# Create a temporary directory to store downloaded files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "temp_files = []\n",
    "dfs = []\n",
    "\n",
    "try:\n",
    "    # Download all files first\n",
    "    print(\"Downloading files from GCS...\")\n",
    "    for file_path in parquet_files:\n",
    "        try:\n",
    "            blob = bucket.blob(file_path)\n",
    "            temp_file_path = os.path.join(temp_dir, os.path.basename(file_path))\n",
    "            blob.download_to_filename(temp_file_path)\n",
    "            temp_files.append(temp_file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error downloading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not temp_files:\n",
    "        raise ValueError(\"No parquet files were successfully downloaded\")\n",
    "    \n",
    "    print(f\"Downloaded {len(temp_files)} files\")\n",
    "    print(\"Loading files with Pandas (to handle timestamp types)...\")\n",
    "    \n",
    "    # Load files with Pandas first (handles nanosecond timestamps better)\n",
    "    for temp_file in temp_files:\n",
    "        try:\n",
    "            df_temp = pd.read_parquet(temp_file)\n",
    "            dfs.append(df_temp)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error loading {temp_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No parquet files were successfully loaded\")\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    print(\"Combining DataFrames...\")\n",
    "    pandas_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Convert numpy arrays/lists to Python lists for Spark compatibility\n",
    "    print(\"Converting data types for Spark compatibility...\")\n",
    "    for col_name in pandas_df.columns:\n",
    "        if pandas_df[col_name].dtype == 'object':\n",
    "            # Check if column contains arrays/lists\n",
    "            sample = pandas_df[col_name].dropna().iloc[0] if len(pandas_df[col_name].dropna()) > 0 else None\n",
    "            if sample is not None and isinstance(sample, (list, np.ndarray)):\n",
    "                # Convert numpy arrays to Python lists\n",
    "                pandas_df[col_name] = pandas_df[col_name].apply(\n",
    "                    lambda x: x.tolist() if isinstance(x, np.ndarray) else (list(x) if isinstance(x, (list, tuple)) else x)\n",
    "                )\n",
    "    \n",
    "    # Drop columns that aren't needed for the pipeline (like 'topics' if it exists)\n",
    "    columns_to_drop = ['topics']  # Add any other columns that cause issues\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in pandas_df.columns]\n",
    "    if columns_to_drop:\n",
    "        print(f\"Dropping columns not needed for pipeline: {columns_to_drop}\")\n",
    "        pandas_df = pandas_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    print(\"Converting to Spark DataFrame...\")\n",
    "    news_df = spark.createDataFrame(pandas_df)\n",
    "    \n",
    "    # Force evaluation and cache to ensure data is loaded in memory before cleanup\n",
    "    print(\"Caching DataFrame in memory...\")\n",
    "    news_df = news_df.cache()\n",
    "    record_count = news_df.count()\n",
    "    \n",
    "    print(f\"✅ Loaded {record_count:,} articles\")\n",
    "    print()\n",
    "    print(\"Schema:\")\n",
    "    news_df.printSchema()\n",
    "    print()\n",
    "    print(\"Sample data:\")\n",
    "    news_df.show(5, truncate=50)\n",
    "    \n",
    "finally:\n",
    "    # Clean up temporary directory\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"\\n✅ Cleaned up temporary files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948abe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA VALIDATION\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 07:58:10 WARN TaskSetManager: Stage 6 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/14 07:58:10 WARN TaskSetManager: Stage 9 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/14 07:58:10 WARN TaskSetManager: Stage 12 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/14 07:58:10 WARN TaskSetManager: Stage 15 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/14 07:58:10 WARN TaskSetManager: Stage 18 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/14 07:58:11 WARN TaskSetManager: Stage 21 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "  null_ticker: ✅ PASS\n",
      "  null_date: ✅ PASS\n",
      "  null_title: ✅ PASS\n",
      "  empty_title: ✅ PASS\n",
      "  invalid_dates: ✅ PASS\n",
      "  duplicate_title_date: ⚠️  1219 issues found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 07:58:12 WARN TaskSetManager: Stage 27 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/14 07:58:12 WARN TaskSetManager: Stage 30 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid records: 23,359 / 23,359\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 3: Data Validation (Production-Level)\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Schema Validation\n",
    "required_columns = ['ticker', 'query_date', 'title']\n",
    "missing_columns = [col for col in required_columns if col not in news_df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# 2. Data Quality Checks\n",
    "validation_results = {}\n",
    "\n",
    "validation_results['null_ticker'] = news_df.filter(col('ticker').isNull()).count()\n",
    "validation_results['null_date'] = news_df.filter(col('query_date').isNull()).count()\n",
    "validation_results['null_title'] = news_df.filter(col('title').isNull()).count()\n",
    "validation_results['empty_title'] = news_df.filter(\n",
    "    (col('title').isNull()) | (trim(col('title')) == '')\n",
    ").count()\n",
    "\n",
    "# Check date range validity (last 2 years to future dates)\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "two_years_ago = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')\n",
    "validation_results['invalid_dates'] = news_df.filter(\n",
    "    (col('query_date') < two_years_ago) | (col('query_date') > current_date)\n",
    ").count()\n",
    "\n",
    "# Check for duplicate articles\n",
    "if 'url' in news_df.columns:\n",
    "    validation_results['duplicate_urls'] = news_df.groupBy('url').count().filter(col('count') > 1).count()\n",
    "else:\n",
    "    validation_results['duplicate_title_date'] = news_df.groupBy('title', 'query_date').count().filter(col('count') > 1).count()\n",
    "\n",
    "# Print validation results\n",
    "print(\"Validation Results:\")\n",
    "for check, result in validation_results.items():\n",
    "    status = \"✅ PASS\" if result == 0 else f\"⚠️  {result} issues found\"\n",
    "    print(f\"  {check}: {status}\")\n",
    "\n",
    "# Filter out invalid records\n",
    "valid_df = news_df.filter(\n",
    "    col('ticker').isNotNull() &\n",
    "    col('query_date').isNotNull() &\n",
    "    col('title').isNotNull() &\n",
    "    (trim(col('title')) != '')\n",
    ")\n",
    "\n",
    "print(f\"\\nValid records: {valid_df.count():,} / {news_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f2ad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA TRANSFORMATION\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 07:58:58 WARN TaskSetManager: Stage 33 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformed 23,359 articles\n",
      "Sample data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+------------+-----------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+----+-------------+-------------+-------------------+--------------+\n",
      "|ticker|company_name|  date_key|article_date|query_date_parsed|                                             title|                                              text|                                    sentiment_text| url|search_source|endpoint_used|ingestion_timestamp|   data_source|\n",
      "+------+------------+----------+------------+-----------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+----+-------------+-------------+-------------------+--------------+\n",
      "|  AAPL|  Apple Inc.|2024-11-12|  2024-11-12|       2024-11-12|Apple to announce AI wall tablet as soon as Mar...|Apple is planning on launching a wall-mounted d...|Apple to announce AI wall tablet as soon as Mar...|NULL|       ticker|     advanced|2025-11-14 07:58:58|stock_news_api|\n",
      "|  AAPL|  Apple Inc.|2024-11-12|  2024-11-12|       2024-11-12|Apple developing an iPad-like AI device that ca...|The product, code-named J490, could be announce...|Apple developing an iPad-like AI device that ca...|NULL|       ticker|     advanced|2025-11-14 07:58:58|stock_news_api|\n",
      "|  AAPL|  Apple Inc.|2024-11-12|  2024-11-12|       2024-11-12|Apple won't launch a smart ring, says Oura CEO:...|Apple will not introduce a smart ring, the CEO ...|Apple won't launch a smart ring, says Oura CEO:...|NULL|       ticker|     advanced|2025-11-14 07:58:58|stock_news_api|\n",
      "|  AAPL|  Apple Inc.|2024-11-12|  2024-11-12|       2024-11-12|Apple reportedly shipping a security camera in ...|Apple is set to expand its smart home presence,...|Apple reportedly shipping a security camera in ...|NULL|       ticker|     advanced|2025-11-14 07:58:58|stock_news_api|\n",
      "|  AAPL|  Apple Inc.|2024-11-12|  2024-11-12|       2024-11-12|Apple reportedly releasing a wall-mounted smart...|Apple is gearing up to announce a new smart hom...|Apple reportedly releasing a wall-mounted smart...|NULL|       ticker|     advanced|2025-11-14 07:58:58|stock_news_api|\n",
      "+------+------------+----------+------------+-----------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+----+-------------+-------------+-------------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 4: Data Transformation and Cleaning\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA TRANSFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Standardize date columns\n",
    "transformed_df = valid_df.withColumn(\n",
    "    'article_date',\n",
    "    to_date(col('date'), 'yyyy-MM-dd')\n",
    ").withColumn(\n",
    "    'query_date_parsed',\n",
    "    to_date(col('query_date'), 'yyyy-MM-dd')\n",
    ").withColumn(\n",
    "    'date_key',  # For joining with price data later\n",
    "    date_format(col('query_date_parsed'), 'yyyy-MM-dd')\n",
    ")\n",
    "\n",
    "# 2. Clean text fields\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    'title_clean',\n",
    "    trim(lower(regexp_replace(col('title'), r'[^\\w\\s]', '')))\n",
    ").withColumn(\n",
    "    'text_clean',\n",
    "    when(col('text').isNotNull(), \n",
    "         trim(lower(regexp_replace(col('text'), r'[^\\w\\s]', ''))))\n",
    "    .otherwise(lit(''))\n",
    ")\n",
    "\n",
    "# 3. Extract text for sentiment analysis (title + text if available)\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    'sentiment_text',\n",
    "    when(col('text').isNotNull() & (trim(col('text')) != ''),\n",
    "         concat_ws(' ', col('title'), col('text')))\n",
    "    .otherwise(col('title'))\n",
    ")\n",
    "\n",
    "# 4. Ensure ticker is uppercase\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    'ticker',\n",
    "    upper(col('ticker'))\n",
    ")\n",
    "\n",
    "# 5. Add metadata columns\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    'ingestion_timestamp',\n",
    "    lit(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    ").withColumn(\n",
    "    'data_source',\n",
    "    lit('stock_news_api')\n",
    ")\n",
    "\n",
    "# 6. Select and order columns for final schema\n",
    "final_columns = [\n",
    "    'ticker',\n",
    "    'company_name',\n",
    "    'date_key',\n",
    "    'article_date',\n",
    "    'query_date_parsed',\n",
    "    'title',\n",
    "    'text',\n",
    "    'sentiment_text',\n",
    "    'url',\n",
    "    'search_source',\n",
    "    'endpoint_used',\n",
    "    'ingestion_timestamp',\n",
    "    'data_source'\n",
    "]\n",
    "\n",
    "# Add any additional columns that exist\n",
    "for col_name in final_columns:\n",
    "    if col_name not in transformed_df.columns:\n",
    "        transformed_df = transformed_df.withColumn(col_name, lit(None))\n",
    "\n",
    "transformed_df = transformed_df.select(*final_columns)\n",
    "\n",
    "print(f\"✅ Transformed {transformed_df.count():,} articles\")\n",
    "print(\"Sample data:\")\n",
    "transformed_df.show(5, truncate=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f005943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING DATA FOR SENTIMENT ANALYSIS\n",
      "======================================================================\n",
      "Converting Spark DataFrame to Pandas for sentiment analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 07:59:52 WARN TaskSetManager: Stage 37 contains a task of very large size (1019 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 23,359 articles for sentiment analysis\n",
      "\n",
      "Sample text length statistics:\n",
      "count    23359.000000\n",
      "mean       299.225609\n",
      "std        139.688319\n",
      "min         44.000000\n",
      "25%        210.000000\n",
      "50%        274.000000\n",
      "75%        356.000000\n",
      "max       7721.000000\n",
      "Name: sentiment_text, dtype: float64\n",
      "\n",
      "Sample sentiment text:\n",
      "['Apple to announce AI wall tablet as soon as March, Bloomberg News reports Apple is planning on launching a wall-mounted display that can control appliances, handle video conferencing and use artificial intelligence to navigate apps, Bloomberg News reported on Tuesday, citing people with knowledge of the effort.', 'Apple developing an iPad-like AI device that can be mounted on a wall: report The product, code-named J490, could be announced as early as March, the report said.', \"Apple won't launch a smart ring, says Oura CEO: 'It's hard to do' Apple will not introduce a smart ring, the CEO of health-tech firm Oura told CNBC, despite speculation the iPhone giant may be considering a move into this product category. Samsung's smart ring debut put the products firmly in the spotlight, with one analyst predicting that Apple could launch its own smart ring in 2026.\"]\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 5: Prepare Data for Sentiment Analysis\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PREPARING DATA FOR SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for sentiment analysis\n",
    "# For very large datasets, consider using Spark UDFs or distributed processing\n",
    "print(\"Converting Spark DataFrame to Pandas for sentiment analysis...\")\n",
    "sentiment_df = transformed_df.select(\n",
    "    'ticker', 'date_key', 'sentiment_text', 'title', 'url'\n",
    ").toPandas()\n",
    "\n",
    "print(f\"Prepared {len(sentiment_df):,} articles for sentiment analysis\")\n",
    "print(f\"\\nSample text length statistics:\")\n",
    "print(sentiment_df['sentiment_text'].str.len().describe())\n",
    "print(f\"\\nSample sentiment text:\")\n",
    "print(sentiment_df['sentiment_text'].head(3).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f45f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INITIALIZING SENTIMENT MODELS\n",
      "======================================================================\n",
      "Loading FINbert model (this may take a few minutes on first run)...\n",
      "Loading VADER analyzer...\n",
      "✅ Models loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 4: Initialize Sentiment Analysis Models\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"INITIALIZING SENTIMENT MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize FINbert with safetensors to avoid torch.load vulnerability\n",
    "print(\"Loading FINbert model (this may take a few minutes on first run)...\")\n",
    "try:\n",
    "    finbert_tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "    finbert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"yiyanghkust/finbert-tone\",\n",
    "        use_safetensors=True  # Use safetensors format to avoid torch.load issue\n",
    "    )\n",
    "    finbert_model.eval()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading with safetensors, trying alternative method: {e}\")\n",
    "    # Fallback: try with trust_remote_code\n",
    "    finbert_tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "    finbert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"yiyanghkust/finbert-tone\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    finbert_model.eval()\n",
    "\n",
    "# Initialize VADER\n",
    "print(\"Loading VADER analyzer...\")\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"✅ Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07a8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles to process: 23,359\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total articles to process: {len(sentiment_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125a06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING SENTIMENT ANALYSIS\n",
      "======================================================================\n",
      "Running FINbert analysis (this will take ~2 hours)...\n",
      "Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 23359/23359 [38:54<00:00, 10.00it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running VADER analysis (this will take ~6 minutes)...\n",
      "Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 23359/23359 [00:11<00:00, 1999.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Completed sentiment analysis on 23,359 articles\n",
      "\n",
      "Sentiment distribution (FINbert):\n",
      "finbert_label\n",
      "negative    9374\n",
      "positive    9232\n",
      "neutral     4753\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment distribution percentages:\n",
      "finbert_label\n",
      "negative    40.130143\n",
      "positive    39.522240\n",
      "neutral     20.347618\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 5: Run Sentiment Analysis\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def get_finbert_sentiment(text, tokenizer, model, max_length=512):\n",
    "    \"\"\"Get FINbert sentiment score\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return None, None\n",
    "    \n",
    "    text = str(text)[:max_length]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    labels = ['positive', 'negative', 'neutral']\n",
    "    scores = predictions[0].tolist()\n",
    "    dominant_idx = scores.index(max(scores))\n",
    "    dominant_sentiment = labels[dominant_idx]\n",
    "    compound_score = scores[0] - scores[1]  # positive - negative\n",
    "    \n",
    "    return compound_score, dominant_sentiment\n",
    "\n",
    "def get_vader_sentiment(text, analyzer):\n",
    "    \"\"\"Get VADER sentiment scores\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return None, None, None, None\n",
    "    \n",
    "    scores = analyzer.polarity_scores(str(text))\n",
    "    return scores['compound'], scores['pos'], scores['neu'], scores['neg']\n",
    "\n",
    "# Enable progress bar for pandas apply\n",
    "tqdm.pandas(desc=\"Processing articles\")\n",
    "\n",
    "# Apply sentiment analysis with progress bar\n",
    "print(\"Running FINbert analysis (this will take ~2 hours)...\")\n",
    "print(\"Progress:\")\n",
    "finbert_results = sentiment_df['sentiment_text'].progress_apply(\n",
    "    lambda x: pd.Series(get_finbert_sentiment(x, finbert_tokenizer, finbert_model))\n",
    ")\n",
    "sentiment_df[['finbert_score', 'finbert_label']] = finbert_results\n",
    "\n",
    "print(\"\\nRunning VADER analysis (this will take ~6 minutes)...\")\n",
    "print(\"Progress:\")\n",
    "vader_results = sentiment_df['sentiment_text'].progress_apply(\n",
    "    lambda x: pd.Series(get_vader_sentiment(x, vader_analyzer))\n",
    ")\n",
    "sentiment_df[['vader_compound', 'vader_pos', 'vader_neu', 'vader_neg']] = vader_results\n",
    "\n",
    "sentiment_df['sentiment_timestamp'] = datetime.now()\n",
    "\n",
    "print(f\"\\n✅ Completed sentiment analysis on {len(sentiment_df):,} articles\")\n",
    "print(\"\\nSentiment distribution (FINbert):\")\n",
    "print(sentiment_df['finbert_label'].value_counts())\n",
    "print(\"\\nSentiment distribution percentages:\")\n",
    "print(sentiment_df['finbert_label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0234d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MERGING SENTIMENT SCORES\n",
      "======================================================================\n",
      "Original df shape before deduplication: (23359, 13)\n",
      "Original df shape after deduplication: (4514, 13)\n",
      "Removed 18845 duplicate rows\n",
      "\n",
      "Sentiment df shape before deduplication: (23359, 12)\n",
      "Sentiment df shape after deduplication: (4514, 12)\n",
      "\n",
      "✅ Final dataset: 4,514 articles with sentiment scores\n",
      "Expected: 4,514 articles (should match)\n",
      "\n",
      "Sample data:\n",
      "  ticker    date_key                                              title  \\\n",
      "0   AAPL  2024-11-12  Apple to announce AI wall tablet as soon as Ma...   \n",
      "1   AAPL  2024-11-13  Smart Home Devices Could Boost Apple Stock, An...   \n",
      "2   AAPL  2024-11-14  Apple Just Paid Investors: Here's How Much The...   \n",
      "3   AAPL  2024-11-15  Apple deletes US-funded RFE/RL news app from R...   \n",
      "4   AAPL  2024-11-16  Prediction: This Warren Buffett Stock Will Out...   \n",
      "5   AAPL  2024-11-17  Apple May Stop Vision Pro Production By End of...   \n",
      "6   AAPL  2024-11-18  Here's When the Big iPhone Supercycle Will Sta...   \n",
      "7   AAPL  2024-11-19  Apple says Mac users targeted in zero-day cybe...   \n",
      "8   AAPL  2024-11-20  Apple to urge judge to end US smartphone monop...   \n",
      "9   AAPL  2024-11-21  Apple is reportedly building a more conversati...   \n",
      "\n",
      "   finbert_score finbert_label  vader_compound  \n",
      "0   9.999288e-01      positive          0.4767  \n",
      "1  -9.999999e-01      negative          0.9337  \n",
      "2  -9.981187e-01      negative          0.4939  \n",
      "3   9.995383e-01      positive          0.5106  \n",
      "4  -1.000000e+00      negative         -0.1511  \n",
      "5   1.923042e-07       neutral         -0.2023  \n",
      "6   9.982243e-01      positive          0.0000  \n",
      "7   9.987211e-01      positive          0.7579  \n",
      "8   9.973489e-01      positive         -0.4404  \n",
      "9   9.979245e-01      positive          0.8221  \n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 8: Merge Sentiment Scores Back to Spark DataFrame\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"MERGING SENTIMENT SCORES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert sentiment results back to Spark DataFrame\n",
    "sentiment_spark_df = spark.createDataFrame(sentiment_df)\n",
    "\n",
    "# Join with transformed data\n",
    "final_df = transformed_df.join(\n",
    "    sentiment_spark_df.select(\n",
    "        'ticker', 'date_key', 'url',\n",
    "        'finbert_score', 'finbert_label',\n",
    "        'vader_compound', 'vader_pos', 'vader_neu', 'vader_neg',\n",
    "        'sentiment_timestamp'\n",
    "    ),\n",
    "    on=['ticker', 'date_key', 'url'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"✅ Final dataset: {final_df.count():,} articles with sentiment scores\")\n",
    "print(\"\\nSample data:\")\n",
    "final_df.select('ticker', 'date_key', 'title', 'finbert_score', 'finbert_label', 'vader_compound').show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f17b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DUPLICATE CHECK\n",
      "======================================================================\n",
      "Original df shape: (23359, 13)\n",
      "Sentiment df shape: (23359, 12)\n",
      "Final df shape: (4514, 20)\n",
      "\n",
      "Duplicate check on merge keys:\n",
      "Original df duplicates on (ticker, date_key, url): 18845\n",
      "Sentiment df duplicates on (ticker, date_key, url): 18845\n",
      "\n",
      "Unique combinations:\n",
      "Original df unique (ticker, date_key, url): 4514\n",
      "Sentiment df unique (ticker, date_key, url): 4514\n",
      "======================================================================\n",
      "ARTICLES PER STOCK\n",
      "======================================================================\n",
      "ticker\n",
      "NVDA     365\n",
      "AMZN     357\n",
      "TSLA     357\n",
      "AAPL     345\n",
      "META     345\n",
      "GOOGL    342\n",
      "MSFT     329\n",
      "AMD      321\n",
      "V        295\n",
      "TSM      292\n",
      "INTC     290\n",
      "JPM      254\n",
      "ORCL     231\n",
      "QQQ      196\n",
      "SPY      195\n",
      "dtype: int64\n",
      "\n",
      "Total unique articles: 4,514\n",
      "Average articles per stock: 300.9\n",
      "Number of unique stocks: 15\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 9: Duplicate Check and Summary Statistics\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DUPLICATE CHECK AND SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"Transformed df record count: {transformed_df.count():,}\")\n",
    "print(f\"Sentiment df record count: {len(sentiment_df):,}\")\n",
    "print(f\"Final df record count: {final_df.count():,}\")\n",
    "\n",
    "# Check for duplicate keys in final DataFrame\n",
    "print(f\"\\nDuplicate check on merge keys (ticker, date_key, url):\")\n",
    "duplicate_count = final_df.groupBy('ticker', 'date_key', 'url').count().filter(col('count') > 1).count()\n",
    "print(f\"Duplicate combinations: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {duplicate_count} duplicate (ticker, date_key, url) combinations found!\")\n",
    "    duplicates = final_df.groupBy('ticker', 'date_key', 'url').count().filter(col('count') > 1)\n",
    "    print(\"Sample duplicates:\")\n",
    "    duplicates.show(10)\n",
    "\n",
    "# Check unique combinations\n",
    "print(f\"\\nUnique combinations:\")\n",
    "unique_count = final_df.select('ticker', 'date_key', 'url').distinct().count()\n",
    "print(f\"Unique (ticker, date_key, url) combinations: {unique_count:,}\")\n",
    "\n",
    "# Check articles per stock\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ARTICLES PER STOCK\")\n",
    "print(\"=\" * 70)\n",
    "articles_per_stock = final_df.groupBy('ticker').count().orderBy(col('count').desc())\n",
    "articles_per_stock.show(truncate=False)\n",
    "\n",
    "total_articles = final_df.count()\n",
    "unique_tickers = final_df.select('ticker').distinct().count()\n",
    "print(f\"\\nTotal articles: {total_articles:,}\")\n",
    "print(f\"Average articles per stock: {total_articles / unique_tickers:.1f}\")\n",
    "print(f\"Number of unique stocks: {unique_tickers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a41324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WRITING TO BIGQUERY\n",
      "======================================================================\n",
      "Dropped intermediate columns: ['sentiment_text']\n",
      "Writing 4,514 records to BigQuery...\n",
      "Table: solid-coral-469323-i0.stock_sentiment.news_sentiment_silver\n",
      "Columns being written: ['ticker', 'company_name', 'date_key', 'article_date', 'query_date_parsed', 'title', 'text', 'url', 'search_source', 'endpoint_used', 'ingestion_timestamp', 'data_source', 'finbert_score', 'finbert_label', 'vader_compound', 'vader_pos', 'vader_neu', 'vader_neg', 'sentiment_timestamp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evancallaghan/data_portfolio/data_engineering/stock_x_sentiment/venv311/lib/python3.11/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully wrote 4,514 records to BigQuery\n",
      "✅ Cleaned up temporary file: temp_transformed_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 10: Write to BigQuery\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"WRITING TO BIGQUERY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for BigQuery write\n",
    "print(\"Converting Spark DataFrame to Pandas...\")\n",
    "final_pandas_df = final_df.toPandas()\n",
    "\n",
    "# Drop intermediate columns that aren't in BigQuery schema\n",
    "columns_to_drop = ['sentiment_text', 'title_clean', 'text_clean']  # Add any other intermediate columns\n",
    "columns_to_drop = [col for col in columns_to_drop if col in final_pandas_df.columns]\n",
    "if columns_to_drop:\n",
    "    final_pandas_df = final_pandas_df.drop(columns=columns_to_drop)\n",
    "    print(f\"Dropped intermediate columns: {columns_to_drop}\")\n",
    "\n",
    "# Convert date columns to proper format for BigQuery\n",
    "if 'date_key' in final_pandas_df.columns:\n",
    "    final_pandas_df['date_key'] = pd.to_datetime(final_pandas_df['date_key']).dt.date\n",
    "if 'article_date' in final_pandas_df.columns:\n",
    "    final_pandas_df['article_date'] = pd.to_datetime(final_pandas_df['article_date']).dt.date\n",
    "if 'query_date_parsed' in final_pandas_df.columns:\n",
    "    final_pandas_df['query_date_parsed'] = pd.to_datetime(final_pandas_df['query_date_parsed']).dt.date\n",
    "\n",
    "# Convert timestamp columns\n",
    "if 'ingestion_timestamp' in final_pandas_df.columns:\n",
    "    final_pandas_df['ingestion_timestamp'] = pd.to_datetime(final_pandas_df['ingestion_timestamp'])\n",
    "if 'sentiment_timestamp' in final_pandas_df.columns:\n",
    "    final_pandas_df['sentiment_timestamp'] = pd.to_datetime(final_pandas_df['sentiment_timestamp'])\n",
    "\n",
    "# Write to BigQuery\n",
    "table_id = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}\"\n",
    "\n",
    "print(f\"\\nWriting {len(final_pandas_df):,} records to BigQuery...\")\n",
    "print(f\"Table: {table_id}\")\n",
    "print(f\"Columns being written: {list(final_pandas_df.columns)}\")\n",
    "\n",
    "job = bq_client.load_table_from_dataframe(\n",
    "    final_pandas_df,\n",
    "    table_id,\n",
    "    job_config=bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        create_disposition=bigquery.CreateDisposition.CREATE_NEVER\n",
    "    )\n",
    ")\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "print(f\"✅ Successfully wrote {len(final_pandas_df):,} records to BigQuery\")\n",
    "print(f\"Table: {table_id}\")\n",
    "\n",
    "# Verify write\n",
    "table = bq_client.get_table(table_id)\n",
    "print(f\"\\n✅ Verification:\")\n",
    "print(f\"   Table rows: {table.num_rows:,}\")\n",
    "print(f\"   Table size: {table.num_bytes / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "873a99b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PIPELINE SUMMARY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evancallaghan/data_portfolio/data_engineering/stock_x_sentiment/venv311/lib/python3.11/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_articles  unique_tickers  unique_dates  avg_finbert_score  avg_vader_compound  positive_count  negative_count  neutral_count\n",
      "0            4514              15           366           0.025675            0.323161            1881            1742            891\n",
      "\n",
      "✅ Pipeline completed successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 8: Validation and Summary\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Query BigQuery to verify\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_articles,\n",
    "    COUNT(DISTINCT ticker) as unique_tickers,\n",
    "    COUNT(DISTINCT date_key) as unique_dates,\n",
    "    AVG(finbert_score) as avg_finbert_score,\n",
    "    AVG(vader_compound) as avg_vader_compound,\n",
    "    COUNT(CASE WHEN finbert_label = 'positive' THEN 1 END) as positive_count,\n",
    "    COUNT(CASE WHEN finbert_label = 'negative' THEN 1 END) as negative_count,\n",
    "    COUNT(CASE WHEN finbert_label = 'neutral' THEN 1 END) as neutral_count\n",
    "FROM `{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}`\n",
    "\"\"\"\n",
    "\n",
    "results = bq_client.query(query).to_dataframe()\n",
    "print(results.to_string())\n",
    "\n",
    "print(\"\\n✅ Pipeline completed successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
