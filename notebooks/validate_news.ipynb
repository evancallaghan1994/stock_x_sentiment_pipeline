{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b9a1540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NEWS DATA VALIDATION\n",
      "======================================================================\n",
      "Project Root: /Users/evancallaghan/data_portfolio/data_engineering/stock_x_sentiment\n",
      "GCS Bucket: stock_sentiment_pipeline\n",
      "GCP Project: solid-coral-469323-i0\n",
      "Validation Reports: /Users/evancallaghan/data_portfolio/data_engineering/stock_x_sentiment/validation_reports\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 1: Imports and Configuration\n",
    "# ======================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import storage\n",
    "\n",
    "# Load environment variables\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "project_root = os.path.dirname(script_dir) if '__file__' in globals() else os.path.dirname(os.getcwd())\n",
    "env_path = os.path.join(project_root, \".env\")\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "GCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\")\n",
    "GCP_PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\")\n",
    "BRONZE_NEWS_PATH = \"bronze/news/stock_news_api\"\n",
    "\n",
    "if not GCS_BUCKET_NAME:\n",
    "    raise ValueError(\"GCS_BUCKET_NAME not found in .env file\")\n",
    "if not GCP_PROJECT_ID:\n",
    "    raise ValueError(\"GCP_PROJECT_ID not found in .env file\")\n",
    "\n",
    "# Resolve credentials path if it's relative\n",
    "credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "if credentials_path and not os.path.isabs(credentials_path):\n",
    "    credentials_path = os.path.join(project_root, credentials_path)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "\n",
    "# Create validation_reports directory\n",
    "validation_reports_dir = os.path.join(project_root, \"validation_reports\")\n",
    "os.makedirs(validation_reports_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NEWS DATA VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"GCS Bucket: {GCS_BUCKET_NAME}\")\n",
    "print(f\"GCP Project: {GCP_PROJECT_ID}\")\n",
    "print(f\"Validation Reports: {validation_reports_dir}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18211953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATA FROM GCS\n",
      "======================================================================\n",
      "Finding parquet files...\n",
      "Found 15 parquet files\n",
      "Combining DataFrames...\n",
      "✅ Loaded 23,359 articles\n",
      "Columns: ['news_url', 'image_url', 'title', 'text', 'source_name', 'date', 'topics', 'sentiment', 'type', 'tickers', 'search_source', 'ticker', 'company_name', 'query_date', 'endpoint_used', 'search_terms_used']\n",
      "\n",
      "Sample data:\n",
      "                                            news_url  \\\n",
      "0  https://www.reuters.com/technology/artificial-...   \n",
      "1  https://nypost.com/2024/11/12/business/apple-r...   \n",
      "2  https://www.cnbc.com/2024/11/12/apple-wont-lau...   \n",
      "\n",
      "                                           image_url  \\\n",
      "0  https://cdn.snapi.dev/images/v1/0/u/7/aapl28-2...   \n",
      "1  https://cdn.snapi.dev/images/v1/p/c/b/aapl8-26...   \n",
      "2  https://cdn.snapi.dev/images/v1/c/0/t/aapl30-2...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Apple to announce AI wall tablet as soon as Ma...   \n",
      "1  Apple developing an iPad-like AI device that c...   \n",
      "2  Apple won't launch a smart ring, says Oura CEO...   \n",
      "\n",
      "                                                text    source_name  \\\n",
      "0  Apple is planning on launching a wall-mounted ...        Reuters   \n",
      "1  The product, code-named J490, could be announc...  New York Post   \n",
      "2  Apple will not introduce a smart ring, the CEO...           CNBC   \n",
      "\n",
      "                       date     topics sentiment     type tickers  \\\n",
      "0 2024-11-12 16:30:13-05:00         []  Positive  Article  [AAPL]   \n",
      "1 2024-11-12 19:27:13-05:00  [product]   Neutral  Article  [AAPL]   \n",
      "2 2024-11-12 07:35:34-05:00      [CEO]   Neutral  Article  [AAPL]   \n",
      "\n",
      "  search_source ticker company_name query_date endpoint_used search_terms_used  \n",
      "0        ticker   AAPL   Apple Inc. 2024-11-12      advanced       ticker:AAPL  \n",
      "1        ticker   AAPL   Apple Inc. 2024-11-12      advanced       ticker:AAPL  \n",
      "2        ticker   AAPL   Apple Inc. 2024-11-12      advanced       ticker:AAPL  \n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 2: Load News Data from GCS Bronze Layer\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATA FROM GCS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize GCS client\n",
    "storage_client = storage.Client(project=GCP_PROJECT_ID)\n",
    "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
    "\n",
    "# List all parquet files\n",
    "print(\"Finding parquet files...\")\n",
    "blobs = bucket.list_blobs(prefix=BRONZE_NEWS_PATH)\n",
    "parquet_files = [blob.name for blob in blobs if blob.name.endswith('.parquet')]\n",
    "\n",
    "print(f\"Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "# Download and load files\n",
    "dfs = []\n",
    "for file_path in parquet_files:\n",
    "    try:\n",
    "        blob = bucket.blob(file_path)\n",
    "        # Download to memory\n",
    "        content = blob.download_as_bytes()\n",
    "        # Read from bytes\n",
    "        import io\n",
    "        df_temp = pd.read_parquet(io.BytesIO(content))\n",
    "        dfs.append(df_temp)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error loading {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No parquet files were successfully loaded\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "print(\"Combining DataFrames...\")\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"✅ Loaded {len(df):,} articles\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41add086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING VALIDATION CHECKS\n",
      "======================================================================\n",
      "\n",
      "1. Checking for null dates...\n",
      "   date: ✅ PASS\n",
      "   query_date: ✅ PASS\n",
      "\n",
      "2. Checking date continuity...\n",
      "   Date range: 2024-11-12 00:00:00 to 2025-11-12 00:00:00\n",
      "   Total gaps > 1 day: 589 (expected for news data)\n",
      "   Large gaps > 7 days: 5\n",
      "   Continuity: ⚠️  WARNING (5 large gaps > 7 days)\n",
      "   Note: Large gaps may indicate missing data collection periods\n",
      "\n",
      "3. Checking for duplicates...\n",
      "   (ticker, query_date, news_url): ℹ️  INFO (1994 duplicate records - will be deduplicated in transformation)\n",
      "      Unique duplicate groups: 993\n",
      "      Avg duplicates per group: 2.0\n",
      "      Max duplicates in a group: 3\n",
      "      Note: Duplicates are expected (same URL found via different search terms)\n",
      "            They will be removed during the transformation step\n",
      "   (ticker, query_date, title): ℹ️  INFO (1994 duplicates - may be legitimate)\n",
      "\n",
      "4. Checking column types...\n",
      "   ticker: ✅ PASS\n",
      "   query_date: ✅ PASS\n",
      "   title: ✅ PASS\n",
      "   text: ✅ PASS\n",
      "\n",
      "5. Checking required fields...\n",
      "   title: ✅ PASS\n",
      "   ticker: ✅ PASS\n",
      "\n",
      "6. Data quality summary...\n",
      "   Unique tickers: 15\n",
      "   Unique dates: 366\n",
      "   Avg articles per ticker: 1557.3\n",
      "\n",
      "======================================================================\n",
      "OVERALL VALIDATION STATUS: PASS\n",
      "======================================================================\n",
      "Note: Date continuity gaps and duplicates are informational only\n",
      "      (Duplicates will be handled during transformation)\n",
      "      Critical checks: null dates, required fields\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 3: Run Validation Checks\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING VALIDATION CHECKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_results = {\n",
    "    \"validation_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"data_source\": \"news\",\n",
    "    \"total_records\": len(df),\n",
    "    \"checks\": {}\n",
    "}\n",
    "\n",
    "# 1. Check for null dates\n",
    "print(\"\\n1. Checking for null dates...\")\n",
    "date_columns = ['date', 'query_date', 'article_date', 'query_date_parsed', 'date_key']\n",
    "null_date_checks = {}\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_date_checks[col] = {\n",
    "            \"null_count\": int(null_count),\n",
    "            \"null_percentage\": float(null_count / len(df) * 100) if len(df) > 0 else 0.0,\n",
    "            \"passed\": null_count == 0\n",
    "        }\n",
    "        status = \"✅ PASS\" if null_count == 0 else f\"⚠️  FAIL ({null_count} nulls)\"\n",
    "        print(f\"   {col}: {status}\")\n",
    "\n",
    "validation_results[\"checks\"][\"null_dates\"] = null_date_checks\n",
    "\n",
    "# 2. Check date continuity (for query_date)\n",
    "# Note: For news data, gaps are expected (weekends, holidays, no news days)\n",
    "# We'll flag only large gaps (> 7 days) as potential issues\n",
    "print(\"\\n2. Checking date continuity...\")\n",
    "if 'query_date' in df.columns:\n",
    "    # Convert to datetime if string\n",
    "    if df['query_date'].dtype == 'object':\n",
    "        df['query_date'] = pd.to_datetime(df['query_date'], errors='coerce')\n",
    "    \n",
    "    # Get date range\n",
    "    min_date = df['query_date'].min()\n",
    "    max_date = df['query_date'].max()\n",
    "    \n",
    "    # Check for large gaps (group by ticker and date)\n",
    "    # Only flag gaps > 7 days (more than a week)\n",
    "    continuity_issues = []\n",
    "    all_gaps = []\n",
    "    for ticker in df['ticker'].unique() if 'ticker' in df.columns else [None]:\n",
    "        ticker_df = df[df['ticker'] == ticker] if ticker else df\n",
    "        dates = ticker_df['query_date'].dropna().dt.date.unique()\n",
    "        dates_sorted = sorted(dates)\n",
    "        \n",
    "        if len(dates_sorted) > 1:\n",
    "            # Check for gaps larger than 7 days\n",
    "            for i in range(len(dates_sorted) - 1):\n",
    "                gap = (dates_sorted[i+1] - dates_sorted[i]).days\n",
    "                all_gaps.append(gap)\n",
    "                if gap > 7:  # Flag gaps larger than a week\n",
    "                    continuity_issues.append({\n",
    "                        \"ticker\": ticker,\n",
    "                        \"gap_start\": str(dates_sorted[i]),\n",
    "                        \"gap_end\": str(dates_sorted[i+1]),\n",
    "                        \"gap_days\": gap\n",
    "                    })\n",
    "    \n",
    "    validation_results[\"checks\"][\"date_continuity\"] = {\n",
    "        \"min_date\": str(min_date) if pd.notna(min_date) else None,\n",
    "        \"max_date\": str(max_date) if pd.notna(max_date) else None,\n",
    "        \"date_range_days\": (max_date - min_date).days if pd.notna(min_date) and pd.notna(max_date) else None,\n",
    "        \"total_gaps_1day\": len([g for g in all_gaps if g > 1]),\n",
    "        \"total_gaps_7day\": len(continuity_issues),\n",
    "        \"continuity_issues\": continuity_issues,\n",
    "        \"passed\": len(continuity_issues) == 0  # Only fail on gaps > 7 days\n",
    "    }\n",
    "    \n",
    "    status = \"✅ PASS\" if len(continuity_issues) == 0 else f\"⚠️  WARNING ({len(continuity_issues)} large gaps > 7 days)\"\n",
    "    print(f\"   Date range: {min_date} to {max_date}\")\n",
    "    print(f\"   Total gaps > 1 day: {len([g for g in all_gaps if g > 1])} (expected for news data)\")\n",
    "    print(f\"   Large gaps > 7 days: {len(continuity_issues)}\")\n",
    "    print(f\"   Continuity: {status}\")\n",
    "    if continuity_issues:\n",
    "        print(f\"   Note: Large gaps may indicate missing data collection periods\")\n",
    "\n",
    "# 3. Check for duplicates\n",
    "print(\"\\n3. Checking for duplicates...\")\n",
    "duplicate_checks = {}\n",
    "\n",
    "# Primary check: duplicate (ticker, query_date, url) combinations\n",
    "# This is the most reliable check since URLs should be unique\n",
    "# Use 'news_url' if 'url' doesn't exist\n",
    "url_col = 'url' if 'url' in df.columns else ('news_url' if 'news_url' in df.columns else None)\n",
    "if url_col and 'ticker' in df.columns and 'query_date' in df.columns:\n",
    "    duplicates = df.duplicated(subset=['ticker', 'query_date', url_col], keep=False)\n",
    "    duplicate_count = duplicates.sum()\n",
    "    \n",
    "    # Get more details about duplicates\n",
    "    duplicate_details = {}\n",
    "    if duplicate_count > 0:\n",
    "        duplicate_df = df[duplicates].copy()\n",
    "        # Count unique duplicate groups\n",
    "        duplicate_groups = duplicate_df.groupby(['ticker', 'query_date', url_col]).size()\n",
    "        duplicate_details = {\n",
    "            \"unique_duplicate_groups\": int(len(duplicate_groups)),\n",
    "            \"avg_duplicates_per_group\": float(duplicate_groups.mean()) if len(duplicate_groups) > 0 else 0.0,\n",
    "            \"max_duplicates_in_group\": int(duplicate_groups.max()) if len(duplicate_groups) > 0 else 0,\n",
    "            \"sample_duplicates\": []\n",
    "        }\n",
    "        \n",
    "        # Show sample of duplicate groups\n",
    "        sample_groups = duplicate_groups.head(5)\n",
    "        for (ticker, date, url), count in sample_groups.items():\n",
    "            duplicate_details[\"sample_duplicates\"].append({\n",
    "                \"ticker\": str(ticker),\n",
    "                \"query_date\": str(date),\n",
    "                \"url\": str(url)[:100] + \"...\" if len(str(url)) > 100 else str(url),\n",
    "                \"count\": int(count)\n",
    "            })\n",
    "    \n",
    "    duplicate_checks[\"ticker_date_url\"] = {\n",
    "        \"duplicate_count\": int(duplicate_count),\n",
    "        \"duplicate_details\": duplicate_details if duplicate_count > 0 else None,\n",
    "        \"passed\": True,  # Don't fail - duplicates will be handled in transformation\n",
    "        \"note\": \"Duplicates are expected in Bronze layer and will be deduplicated during transformation\"\n",
    "    }\n",
    "    status = \"✅ PASS\" if duplicate_count == 0 else f\"ℹ️  INFO ({duplicate_count} duplicate records - will be deduplicated in transformation)\"\n",
    "    print(f\"   (ticker, query_date, {url_col}): {status}\")\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"      Unique duplicate groups: {duplicate_details.get('unique_duplicate_groups', 0)}\")\n",
    "        print(f\"      Avg duplicates per group: {duplicate_details.get('avg_duplicates_per_group', 0):.1f}\")\n",
    "        print(f\"      Max duplicates in a group: {duplicate_details.get('max_duplicates_in_group', 0)}\")\n",
    "        print(f\"      Note: Duplicates are expected (same URL found via different search terms)\")\n",
    "        print(f\"            They will be removed during the transformation step\")\n",
    "elif not url_col:\n",
    "    print(f\"   ⚠️  WARNING: No URL column found (neither 'url' nor 'news_url')\")\n",
    "    duplicate_checks[\"ticker_date_url\"] = {\n",
    "        \"duplicate_count\": 0,\n",
    "        \"passed\": True,\n",
    "        \"note\": \"URL column not found - skipping duplicate check\"\n",
    "    }\n",
    "\n",
    "# Secondary check: duplicate (ticker, query_date, title) combinations\n",
    "# This is informational - same titles can appear legitimately on different days\n",
    "if all(col in df.columns for col in ['ticker', 'query_date', 'title']):\n",
    "    duplicates = df.duplicated(subset=['ticker', 'query_date', 'title'], keep=False)\n",
    "    duplicate_count = duplicates.sum()\n",
    "    duplicate_checks[\"ticker_date_title\"] = {\n",
    "        \"duplicate_count\": int(duplicate_count),\n",
    "        \"passed\": True,  # Don't fail on this - it's informational\n",
    "        \"note\": \"Same title on same date may be legitimate (e.g., republished articles)\"\n",
    "    }\n",
    "    status = f\"ℹ️  INFO ({duplicate_count} duplicates - may be legitimate)\"\n",
    "    print(f\"   (ticker, query_date, title): {status}\")\n",
    "\n",
    "validation_results[\"checks\"][\"duplicates\"] = duplicate_checks\n",
    "\n",
    "# 4. Check expected column types\n",
    "print(\"\\n4. Checking column types...\")\n",
    "expected_types = {\n",
    "    'ticker': 'object',  # string\n",
    "    'query_date': 'datetime64[ns]',\n",
    "    'title': 'object',  # string\n",
    "    'text': 'object',  # string (nullable)\n",
    "    'url': 'object',  # string (nullable)\n",
    "    'date_key': 'object'  # string\n",
    "}\n",
    "\n",
    "type_checks = {}\n",
    "for col, expected_type in expected_types.items():\n",
    "    if col in df.columns:\n",
    "        actual_type = str(df[col].dtype)\n",
    "        # Normalize type comparison\n",
    "        type_match = (\n",
    "            (expected_type == 'object' and actual_type == 'object') or\n",
    "            (expected_type == 'datetime64[ns]' and 'datetime' in actual_type) or\n",
    "            actual_type == expected_type\n",
    "        )\n",
    "        type_checks[col] = {\n",
    "            \"expected\": expected_type,\n",
    "            \"actual\": actual_type,\n",
    "            \"passed\": type_match\n",
    "        }\n",
    "        status = \"✅ PASS\" if type_match else f\"⚠️  FAIL (expected {expected_type}, got {actual_type})\"\n",
    "        print(f\"   {col}: {status}\")\n",
    "\n",
    "validation_results[\"checks\"][\"column_types\"] = type_checks\n",
    "\n",
    "# 5. Check required fields (title should not be empty)\n",
    "print(\"\\n5. Checking required fields...\")\n",
    "required_field_checks = {}\n",
    "\n",
    "if 'title' in df.columns:\n",
    "    empty_titles = (df['title'].isnull() | (df['title'].astype(str).str.strip() == '')).sum()\n",
    "    required_field_checks[\"title\"] = {\n",
    "        \"empty_count\": int(empty_titles),\n",
    "        \"empty_percentage\": float(empty_titles / len(df) * 100) if len(df) > 0 else 0.0,\n",
    "        \"passed\": empty_titles == 0\n",
    "    }\n",
    "    status = \"✅ PASS\" if empty_titles == 0 else f\"⚠️  FAIL ({empty_titles} empty titles)\"\n",
    "    print(f\"   title: {status}\")\n",
    "\n",
    "if 'ticker' in df.columns:\n",
    "    null_tickers = df['ticker'].isnull().sum()\n",
    "    required_field_checks[\"ticker\"] = {\n",
    "        \"null_count\": int(null_tickers),\n",
    "        \"null_percentage\": float(null_tickers / len(df) * 100) if len(df) > 0 else 0.0,\n",
    "        \"passed\": null_tickers == 0\n",
    "    }\n",
    "    status = \"✅ PASS\" if null_tickers == 0 else f\"⚠️  FAIL ({null_tickers} null tickers)\"\n",
    "    print(f\"   ticker: {status}\")\n",
    "\n",
    "validation_results[\"checks\"][\"required_fields\"] = required_field_checks\n",
    "\n",
    "# 6. Data quality summary\n",
    "print(\"\\n6. Data quality summary...\")\n",
    "unique_tickers = df['ticker'].nunique() if 'ticker' in df.columns else 0\n",
    "unique_dates = df['query_date'].nunique() if 'query_date' in df.columns else 0\n",
    "\n",
    "validation_results[\"summary\"] = {\n",
    "    \"unique_tickers\": int(unique_tickers),\n",
    "    \"unique_dates\": int(unique_dates),\n",
    "    \"articles_per_ticker_avg\": float(len(df) / unique_tickers) if unique_tickers > 0 else 0.0\n",
    "}\n",
    "\n",
    "print(f\"   Unique tickers: {unique_tickers}\")\n",
    "print(f\"   Unique dates: {unique_dates}\")\n",
    "print(f\"   Avg articles per ticker: {len(df) / unique_tickers:.1f}\" if unique_tickers > 0 else \"   Avg articles per ticker: N/A\")\n",
    "\n",
    "# Overall validation status\n",
    "# Only fail on critical checks: null dates and required fields\n",
    "# Duplicates are expected in Bronze layer and will be handled in transformation\n",
    "critical_checks = []\n",
    "\n",
    "# Add null date checks\n",
    "if \"null_dates\" in validation_results[\"checks\"]:\n",
    "    for col_check in validation_results[\"checks\"][\"null_dates\"].values():\n",
    "        if isinstance(col_check, dict):\n",
    "            critical_checks.append(col_check)\n",
    "\n",
    "# Add required field checks\n",
    "if \"required_fields\" in validation_results[\"checks\"]:\n",
    "    for field_check in validation_results[\"checks\"][\"required_fields\"].values():\n",
    "        if isinstance(field_check, dict):\n",
    "            critical_checks.append(field_check)\n",
    "\n",
    "# Note: URL duplicates are NOT critical - they're expected and will be deduplicated in transformation\n",
    "\n",
    "all_passed = all(\n",
    "    check.get(\"passed\", False) \n",
    "    for check in critical_checks\n",
    "    if isinstance(check, dict) and check.get(\"passed\") is not None\n",
    ")\n",
    "\n",
    "validation_results[\"overall_status\"] = \"PASS\" if all_passed else \"FAIL\"\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OVERALL VALIDATION STATUS: {validation_results['overall_status']}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Note: Date continuity gaps and duplicates are informational only\")\n",
    "print(\"      (Duplicates will be handled during transformation)\")\n",
    "print(\"      Critical checks: null dates, required fields\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d099b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING VALIDATION REPORT\n",
      "======================================================================\n",
      "✅ Validation report saved to: /Users/evancallaghan/data_portfolio/data_engineering/stock_x_sentiment/validation_reports/20251114_news.json\n",
      "\n",
      "Report Summary:\n",
      "  Total Records: 23,359\n",
      "  Overall Status: PASS\n",
      "  Checks Performed: 5\n",
      "\n",
      "======================================================================\n",
      "VALIDATION REPORT\n",
      "======================================================================\n",
      "{\n",
      "  \"validation_date\": \"2025-11-14 08:28:25\",\n",
      "  \"data_source\": \"news\",\n",
      "  \"total_records\": 23359,\n",
      "  \"checks\": {\n",
      "    \"null_dates\": {\n",
      "      \"date\": {\n",
      "        \"null_count\": 0,\n",
      "        \"null_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      },\n",
      "      \"query_date\": {\n",
      "        \"null_count\": 0,\n",
      "        \"null_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      }\n",
      "    },\n",
      "    \"date_continuity\": {\n",
      "      \"min_date\": \"2024-11-12 00:00:00\",\n",
      "      \"max_date\": \"2025-11-12 00:00:00\",\n",
      "      \"date_range_days\": 365,\n",
      "      \"total_gaps_1day\": 589,\n",
      "      \"total_gaps_7day\": 5,\n",
      "      \"continuity_issues\": [\n",
      "        {\n",
      "          \"ticker\": \"ORCL\",\n",
      "          \"gap_start\": \"2025-04-16\",\n",
      "          \"gap_end\": \"2025-04-25\",\n",
      "          \"gap_days\": 9\n",
      "        },\n",
      "        {\n",
      "          \"ticker\": \"SPY\",\n",
      "          \"gap_start\": \"2024-12-06\",\n",
      "          \"gap_end\": \"2024-12-17\",\n",
      "          \"gap_days\": 11\n",
      "        },\n",
      "        {\n",
      "          \"ticker\": \"SPY\",\n",
      "          \"gap_start\": \"2025-01-15\",\n",
      "          \"gap_end\": \"2025-01-23\",\n",
      "          \"gap_days\": 8\n",
      "        },\n",
      "        {\n",
      "          \"ticker\": \"SPY\",\n",
      "          \"gap_start\": \"2025-02-03\",\n",
      "          \"gap_end\": \"2025-02-12\",\n",
      "          \"gap_days\": 9\n",
      "        },\n",
      "        {\n",
      "          \"ticker\": \"SPY\",\n",
      "          \"gap_start\": \"2025-08-15\",\n",
      "          \"gap_end\": \"2025-08-27\",\n",
      "          \"gap_days\": 12\n",
      "        }\n",
      "      ],\n",
      "      \"passed\": false\n",
      "    },\n",
      "    \"duplicates\": {\n",
      "      \"ticker_date_url\": {\n",
      "        \"duplicate_count\": 1994,\n",
      "        \"duplicate_details\": {\n",
      "          \"unique_duplicate_groups\": 993,\n",
      "          \"avg_duplicates_per_group\": 2.008056394763343,\n",
      "          \"max_duplicates_in_group\": 3,\n",
      "          \"sample_duplicates\": [\n",
      "            {\n",
      "              \"ticker\": \"AAPL\",\n",
      "              \"query_date\": \"2024-12-13 00:00:00\",\n",
      "              \"url\": \"https://www.investopedia.com/apple-remains-the-top-pick-at-morgan-stanley-which-sees-ai-driving-ipho...\",\n",
      "              \"count\": 2\n",
      "            },\n",
      "            {\n",
      "              \"ticker\": \"AAPL\",\n",
      "              \"query_date\": \"2024-12-13 00:00:00\",\n",
      "              \"url\": \"https://www.investopedia.com/watch-these-apple-price-levels-as-ai-optimism-propels-stock-to-record-h...\",\n",
      "              \"count\": 2\n",
      "            },\n",
      "            {\n",
      "              \"ticker\": \"AAPL\",\n",
      "              \"query_date\": \"2025-02-14 00:00:00\",\n",
      "              \"url\": \"https://247wallst.com/investing/2025/02/14/apple-aapl-is-flailing-with-earths-biggest-smartphone-mar...\",\n",
      "              \"count\": 2\n",
      "            },\n",
      "            {\n",
      "              \"ticker\": \"AAPL\",\n",
      "              \"query_date\": \"2025-02-14 00:00:00\",\n",
      "              \"url\": \"https://seekingalpha.com/article/4758336-apple-q1-reinforces-my-sell-thesis-but-with-one-massive-cav...\",\n",
      "              \"count\": 2\n",
      "            },\n",
      "            {\n",
      "              \"ticker\": \"AAPL\",\n",
      "              \"query_date\": \"2025-02-14 00:00:00\",\n",
      "              \"url\": \"https://www.investopedia.com/what-analysts-think-of-apple-stock-ahead-of-wednesday-product-launch-11...\",\n",
      "              \"count\": 2\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"passed\": true,\n",
      "        \"note\": \"Duplicates are expected in Bronze layer and will be deduplicated during transformation\"\n",
      "      },\n",
      "      \"ticker_date_title\": {\n",
      "        \"duplicate_count\": 1994,\n",
      "        \"passed\": true,\n",
      "        \"note\": \"Same title on same date may be legitimate (e.g., republished articles)\"\n",
      "      }\n",
      "    },\n",
      "    \"column_types\": {\n",
      "      \"ticker\": {\n",
      "        \"expected\": \"object\",\n",
      "        \"actual\": \"object\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"query_date\": {\n",
      "        \"expected\": \"datetime64[ns]\",\n",
      "        \"actual\": \"datetime64[ns]\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"title\": {\n",
      "        \"expected\": \"object\",\n",
      "        \"actual\": \"object\",\n",
      "        \"passed\": true\n",
      "      },\n",
      "      \"text\": {\n",
      "        \"expected\": \"object\",\n",
      "        \"actual\": \"object\",\n",
      "        \"passed\": true\n",
      "      }\n",
      "    },\n",
      "    \"required_fields\": {\n",
      "      \"title\": {\n",
      "        \"empty_count\": 0,\n",
      "        \"empty_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      },\n",
      "      \"ticker\": {\n",
      "        \"null_count\": 0,\n",
      "        \"null_percentage\": 0.0,\n",
      "        \"passed\": \"True\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"summary\": {\n",
      "    \"unique_tickers\": 15,\n",
      "    \"unique_dates\": 366,\n",
      "    \"articles_per_ticker_avg\": 1557.2666666666667\n",
      "  },\n",
      "  \"overall_status\": \"PASS\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CELL 4: Save Validation Report\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING VALIDATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate report filename with current date\n",
    "report_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "report_filename = f\"{report_date}_news.json\"\n",
    "report_path = os.path.join(validation_reports_dir, report_filename)\n",
    "\n",
    "# Save validation report\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(validation_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✅ Validation report saved to: {report_path}\")\n",
    "print(f\"\\nReport Summary:\")\n",
    "print(f\"  Total Records: {validation_results['total_records']:,}\")\n",
    "print(f\"  Overall Status: {validation_results['overall_status']}\")\n",
    "print(f\"  Checks Performed: {len(validation_results['checks'])}\")\n",
    "\n",
    "# Display validation results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"VALIDATION REPORT\")\n",
    "print(f\"{'='*70}\")\n",
    "print(json.dumps(validation_results, indent=2, default=str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
